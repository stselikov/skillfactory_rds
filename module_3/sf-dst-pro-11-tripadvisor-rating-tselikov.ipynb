{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.pata.org/wp-content/uploads/2014/09/TripAdvisor_Logo-300x119.png)\n# Predict TripAdvisor Rating\n## В этом соревновании нам предстоит предсказать рейтинг ресторана в TripAdvisor\n**По ходу задачи:**\n* Прокачаем работу с pandas\n* Научимся работать с Kaggle Notebooks\n* Поймем как делать предобработку различных данных\n* Научимся работать с пропущенными данными (Nan)\n* Познакомимся с различными видами кодирования признаков\n* Немного попробуем [Feature Engineering](https://ru.wikipedia.org/wiki/Конструирование_признаков) (генерировать новые признаки)\n* И совсем немного затронем ML\n* И многое другое...   \n\n\n\n### И самое важное, все это вы сможете сделать самостоятельно!\n\n*Этот Ноутбук являетсся Примером/Шаблоном к этому соревнованию (Baseline) и не служит готовым решением!*   \nВы можете использовать его как основу для построения своего решения.\n\n> что такое baseline решение, зачем оно нужно и почему предоставлять baseline к соревнованию стало важным стандартом на kaggle и других площадках.   \n**baseline** создается больше как шаблон, где можно посмотреть как происходит обращение с входящими данными и что нужно получить на выходе. При этом МЛ начинка может быть достаточно простой, просто для примера. Это помогает быстрее приступить к самому МЛ, а не тратить ценное время на чисто инженерные задачи. \nТакже baseline являеться хорошей опорной точкой по метрике. Если твое решение хуже baseline - ты явно делаешь что-то не то и стоит попробовать другой путь) \n\nВ контексте нашего соревнования baseline идет с небольшими примерами того, что можно делать с данными, и с инструкцией, что делать дальше, чтобы улучшить результат.  Вообще готовым решением это сложно назвать, так как используются всего 2 самых простых признака (а остальные исключаются)."},{"metadata":{},"cell_type":"markdown","source":"# import"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport collections\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Reviews[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видим, большинство признаков у нас требует очистки и предварительной обработки."},{"metadata":{},"cell_type":"markdown","source":"# Cleaning and Prepping Data\nОбычно данные содержат в себе кучу мусора, который необходимо почистить, для того чтобы привести их в приемлемый формат. Чистка данных — это необходимый этап решения почти любой реальной задачи.   \n![](https://analyticsindiamag.com/wp-content/uploads/2018/01/data-cleaning.png)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Обработка NAN \nУ наличия пропусков могут быть разные причины, но пропуски нужно либо заполнить, либо исключить из набора полностью. Но с пропусками нужно быть внимательным, **даже отсутствие информации может быть важным признаком!**   \nПо этому перед обработкой NAN лучше вынести информацию о наличии пропуска как отдельный признак "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Для примера я возьму столбец Number of Reviews\ndata['Number_of_Reviews_isNAN'] = pd.isna(data['Number of Reviews']).astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number_of_Reviews_isNAN']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Далее заполняем пропуски 0, вы можете попробовать заполнением средним или средним по городу и тд...\ndata['Number of Reviews'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Обработка признаков\nДля начала посмотрим какие признаки у нас могут быть категориальными."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Какие признаки можно считать категориальными?"},{"metadata":{},"cell_type":"markdown","source":"Для кодирования категориальных признаков есть множество подходов:\n* Label Encoding\n* One-Hot Encoding\n* Target Encoding\n* Hashing\n\nВыбор кодирования зависит от признака и выбраной модели.\nНе будем сейчас сильно погружаться в эту тематику, давайте посмотрим лучше пример с One-Hot Encoding:\n![](https://i.imgur.com/mtimFxh.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\n#data = pd.get_dummies(data, columns=[ 'City',], dummy_na=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Возьмем следующий признак \"Price Range\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"По описанию 'Price Range' это - Цены в ресторане.  \nИх можно поставить по возрастанию (значит это не категориальный признак). А это значит, что их можно заменить последовательными числами, например 1,2,3  \n*Попробуйте сделать обработку этого признака уже самостоятельно!*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ваша обработка 'Price Range'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range'] = data['Price Range'].fillna('Price_N')\npr_dict = { '$' : 'Price_C',\n          '$$ - $$$' : 'Price_B',\n          '$$$$' : 'Price_A'}\ndata['Price Range'] = data['Price Range'].replace(to_replace=pr_dict)\ndata = pd.concat([data, pd.get_dummies(data['Price Range'])], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Для некоторых алгоритмов МЛ даже для не категориальных признаков можно применить One-Hot Encoding, и это может улучшить качество модели. Пробуйте разные подходы к кодированию признака - никто не знает заранее, что может взлететь."},{"metadata":{},"cell_type":"markdown","source":"### Обработать другие признаки вы должны самостоятельно!\nДля обработки других признаков вам возможно придется даже написать свою функцию, а может даже и не одну, но в этом и есть ваша практика в этом модуле!     \nСледуя подсказкам в модуле вы сможете более подробно узнать, как сделать эти приобразования."},{"metadata":{"trusted":true},"cell_type":"code","source":"# тут ваш код на обработку других признаков\n# .....\n\ndata['Cuisine Style'] = data['Cuisine Style'].fillna('[\\'Unknown\\']')\ndata['Cuisine Style'] = data['Cuisine Style'].apply(lambda x: eval(x))\ndata['NoCu'] = data['Cuisine Style'].apply(lambda x: len(x))\ndata['isMono'] = data['NoCu'].apply(lambda x: 1 if x == 1 else 0 )\n#data['isDouble'] = data['NoCu'].apply(lambda x: 1 if x == 2 else 0 )\n#data['isTriple'] = data['NoCu'].apply(lambda x: 1 if x == 3 else 0 )\ndata['isMulti'] = data['NoCu'].apply(lambda x: 1 if x > 5 else 0 )\n\n\ndata = pd.concat([data, pd.get_dummies(data['City'])], axis=1)\n\nCapital_dict = {'Paris' : 1, 'Stockholm' : 1, 'London' : 1, 'Berlin': 1, 'Munich' : 0, 'Oporto': 0,\n       'Milan' : 0, 'Bratislava' : 1, 'Vienna' : 1, 'Rome' : 1, 'Barcelona' : 0, 'Madrid' : 1,\n       'Dublin' : 1, 'Brussels' : 1, 'Zurich' : 0, 'Warsaw' : 1, 'Budapest' : 1, 'Copenhagen': 1,\n       'Amsterdam' : 1, 'Lyon' : 0, 'Hamburg' : 0, 'Lisbon' : 1, 'Prague' : 1, 'Oslo' : 1,\n       'Helsinki': 1, 'Edinburgh': 0, 'Geneva' : 0 , 'Ljubljana' : 1, 'Athens' : 1,\n       'Luxembourg': 1, 'Krakow' : 0}\ndata['Capital'] = data['City'].apply(lambda x: Capital_dict[x])\n\nBig_dict = {'Paris' : 1, 'Stockholm' : 1, 'London' : 1, 'Berlin': 1, 'Munich' : 1, 'Oporto': 0,\n       'Milan' : 1, 'Bratislava' : 0, 'Vienna' : 1, 'Rome' : 1, 'Barcelona' : 1, 'Madrid' : 1,\n       'Dublin' : 0, 'Brussels' : 1, 'Zurich' : 0, 'Warsaw' : 1, 'Budapest' : 1, 'Copenhagen': 1,\n       'Amsterdam' : 0, 'Lyon' : 0, 'Hamburg' : 1, 'Lisbon' : 0, 'Prague' : 1, 'Oslo' : 0,\n       'Helsinki': 0, 'Edinburgh': 0, 'Geneva' : 0 , 'Ljubljana' : 0, 'Athens' : 1,\n       'Luxembourg': 0, 'Krakow' : 0}\ndata['Big_city'] = data['City'].apply(lambda x: Big_dict[x])\n\n\ndfc = (data['Cuisine Style'].explode()).reset_index()\ndfcl = list(dfc['Cuisine Style'].unique())\n\ndef find_item(cell):\n    if item in cell:\n        return 1\n    return 0\nfor item in dfcl:\n    data[item] = data['Cuisine Style'].apply(find_item)\n    \ndata['Reviews'].fillna('[[], []]', inplace=True)\ndef RewFill(s):\n    s = s.replace(\"nan]\", \"'']\")\n    s = s.replace(\"[nan\", \"[''\")\n    if len(s) < 15:\n        return \"[['',''], ['01/01/1900','01/01/1900']]\"\n    return s\ndata['Reviews'] = data['Reviews'].apply(RewFill)\ndata['Reviews'] = data['Reviews'].apply(lambda s: eval(s))\n\ndata['Ans1'] = pd.to_datetime(data['Reviews'].apply(lambda x: x[1][0]))\ndata['Ans2'] = pd.to_datetime(data['Reviews'].apply(lambda x: x[1][1] if (len(x[1]) > 1) else '01/01/1900'))\n\nM1min = data['Ans1'].min()\nM1 = data[data['Ans1'] != M1min]['Ans1'].mean().round('D')\ndata['Ans1'] = data['Ans1'].apply(lambda x: x if x > M1min else M1)\nM2min = data['Ans2'].min().round('D')\nM2 = data[data['Ans2'] != M2min]['Ans2'].mean().round('D')\ndata['Ans2'] = data['Ans2'].apply(lambda x: x if x > M2min else M2)\n\ntoday = pd.to_datetime(\"today\").round('D')\ndata['Ans1'] = data['Ans1'].apply(lambda x: (today - x).days )\ndata['Ans2'] = data['Ans2'].apply(lambda x: (today - x).days )\n#data['DAns'] = (data['Ans1'] - data['Ans2']).apply(abs)\n\n\ndata['Number of Reviews'] = data['Number of Reviews'].fillna(0)\n#data['Number of Reviews'] = data['Number of Reviews'].fillna(data['Number of Reviews'].mean())\n#data['Number of Reviews'] = data['Number of Reviews'].round(-1)\n\ndata = data.drop(['City', 'Cuisine Style', 'Price Range', 'Reviews', 'URL_TA', 'ID_TA'], axis = 1)\n\ndata = data.drop(['Price_N' ], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://cs10.pikabu.ru/post_img/2018/09/06/11/1536261023140110012.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# EDA \n[Exploratory Data Analysis](https://ru.wikipedia.org/wiki/Разведочный_анализ_данных) - Анализ данных\nНа этом этапе мы строим графики, ищем закономерности, аномалии, выбросы или связи между признаками.\nВ общем цель этого этапа понять, что эти данные могут нам дать и как признаки могут быть взаимосвязаны между собой.\nПонимание изначальных признаков позволит сгенерировать новые, более сильные и, тем самым, сделать нашу модель лучше.\n![](https://miro.medium.com/max/2598/1*RXdMb7Uk6mGqWqPguHULaQ.png)"},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение признака"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,7)\ndf_train['Ranking'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"У нас много ресторанов, которые не дотягивают и до 2500 места в своем городе, а что там по городам?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['City'].value_counts(ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"А кто-то говорил, что французы любят поесть=) Посмотрим, как изменится распределение в большом городе:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ranking'][df_train['City'] =='London'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим на топ 10 городов\nfor x in (df_train['City'].value_counts())[0:10].index:\n    df_train['Ranking'][df_train['City'] == x].hist(bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Получается, что Ranking имеет нормальное распределение, просто в больших городах больше ресторанов, из-за мы этого имеем смещение.\n\n>Подумайте как из этого можно сделать признак для вашей модели. Я покажу вам пример, как визуализация помогает находить взаимосвязи. А далее действуйте без подсказок =) \n"},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Rating'].value_counts(ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной относительно признака"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] == 5].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] < 4].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### И один из моих любимых - [корреляция признаков](https://ru.wikipedia.org/wiki/Корреляция)\nНа этом графике уже сейчас вы сможете заметить, как признаки связаны между собой и с целевой переменной."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(data.drop(['sample'], axis=1).corr(),)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Вообще благодаря визуализации в этом датасете можно узнать много интересных фактов, например:\n* где больше Пицерий в Мадриде или Лондоне?\n* в каком городе кухня ресторанов более разнообразна?\n\nпридумайте свои вопрос и найдите на него ответ в данных)"},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nТеперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию."},{"metadata":{"trusted":true},"cell_type":"code","source":"# на всякий случай, заново подгружаем данные\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n    \n    df_output = df_input.copy()\n    \n    # ################### 1. Предобработка ############################################################## \n    # убираем не нужные для модели признаки\n    df_output.drop(['Restaurant_id','ID_TA',], axis = 1, inplace=True)\n    \n    \n    # ################### 2. NAN ############################################################## \n    # Далее заполняем пропуски, вы можете попробовать заполнением средним или средним по городу и тд...\n    df_output['Number of Reviews'].fillna(0, inplace=True)\n    # тут ваш код по обработке NAN\n    # ....\n    \n    \n    # ################### 3. Encoding ############################################################## \n    # для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\n    #df_output = pd.get_dummies(df_output, columns=[ 'City',], dummy_na=True)\n    \n    # тут ваш код не Encoding фитчей\n    # ....\n    \n    \n    # ################### 4. Feature Engineering ####################################################\n    # тут ваш код не генерацию новых фитчей\n    # ....\n    \n    # Обработка признака Price Range, замена его на Dummy - переменные\n    df_output['Price Range'] = df_output['Price Range'].fillna('Price_N')\n    pr_dict = { '$' : 'Price_C',\n          '$$ - $$$' : 'Price_B',\n          '$$$$' : 'Price_A'}\n    df_output['Price Range'] = df_output['Price Range'].replace(to_replace=pr_dict)\n    df_output = pd.concat([df_output, pd.get_dummies(df_output['Price Range'])], axis=1)\n    \n    # df_output = df_output.drop(['Price_N' ], axis = 1)\n    \n    \n    # тут ваш код на обработку других признаков\n    # .....\n    # Обрабатываем Cuisine Style - заменяем строку на список\n    df_output['Cuisine Style'] = df_output['Cuisine Style'].fillna('[\\'Unknown\\']')\n    df_output['Cuisine Style'] = df_output['Cuisine Style'].apply(lambda x: eval(x))\n    \n    # Создаем новый признак - количество кухонь для ресторана\n    df_output['Number Of Cuisines'] = df_output['Cuisine Style'].apply(lambda x: len(x))\n    \n    # Создаем новый признак - ресторан монокухни\n    df_output['Is Mono Cuisine'] = df_output['Number Of Cuisines'].apply(lambda x: 1 if x == 1 else 0 )\n    #df_output['isDouble'] = df_output['Number Of Cuisines'].apply(lambda x: 1 if x == 2 else 0 )\n    #df_output['isTriple'] = df_output['Number Of Cuisines'].apply(lambda x: 1 if x == 3 else 0 )\n    \n    # Создаем новый признак - ресторан с большим количеством кухонь\n    df_output['Is Multi Cuisine'] = df_output['Number Of Cuisines'].apply(lambda x: 1 if x > 5 else 0 )\n\n    # Переводим города в Dummy - Переменные\n    df_output = pd.concat([df_output, pd.get_dummies(data['City'])], axis=1)\n\n    # Создаем признак - город является столицей\n    Capital_dict = {'Paris' : 1, 'Stockholm' : 1, 'London' : 1, 'Berlin': 1, 'Munich' : 0, 'Oporto': 0,\n       'Milan' : 0, 'Bratislava' : 1, 'Vienna' : 1, 'Rome' : 1, 'Barcelona' : 0, 'Madrid' : 1,\n       'Dublin' : 1, 'Brussels' : 1, 'Zurich' : 0, 'Warsaw' : 1, 'Budapest' : 1, 'Copenhagen': 1,\n       'Amsterdam' : 1, 'Lyon' : 0, 'Hamburg' : 0, 'Lisbon' : 1, 'Prague' : 1, 'Oslo' : 1,\n       'Helsinki': 1, 'Edinburgh': 0, 'Geneva' : 0 , 'Ljubljana' : 1, 'Athens' : 1,\n       'Luxembourg': 1, 'Krakow' : 0}\n    df_output['Capital'] = df_output['City'].apply(lambda x: Capital_dict[x])\n\n    # Создаем признак - большой город (население больше миллиона)\n    Big_dict = {'Paris' : 1, 'Stockholm' : 1, 'London' : 1, 'Berlin': 1, 'Munich' : 1, 'Oporto': 0,\n       'Milan' : 1, 'Bratislava' : 0, 'Vienna' : 1, 'Rome' : 1, 'Barcelona' : 1, 'Madrid' : 1,\n       'Dublin' : 0, 'Brussels' : 1, 'Zurich' : 0, 'Warsaw' : 1, 'Budapest' : 1, 'Copenhagen': 1,\n       'Amsterdam' : 0, 'Lyon' : 0, 'Hamburg' : 1, 'Lisbon' : 0, 'Prague' : 1, 'Oslo' : 0,\n       'Helsinki': 0, 'Edinburgh': 0, 'Geneva' : 0 , 'Ljubljana' : 0, 'Athens' : 1,\n       'Luxembourg': 0, 'Krakow' : 0}\n    df_output['Big_city'] = df_output['City'].apply(lambda x: Big_dict[x])\n    \n    # Создаем признак - население города в тысячах человек\n    СityPop_dict= {'London' : 8908, 'Paris' : 2206, 'Madrid' : 3223, 'Barcelona' : 1620, \n                        'Berlin' : 6010, 'Milan' : 1366, 'Rome' : 2872, 'Prague' : 1308, \n                        'Lisbon' : 506, 'Vienna' : 1888, 'Amsterdam' : 860, 'Brussels' : 179, \n                        'Hamburg' : 1841, 'Munich' : 1457, 'Lyon' : 506, 'Stockholm' : 961, \n                        'Budapest' : 1752, 'Warsaw' : 1764, 'Dublin' : 553, \n                        'Copenhagen' : 616, 'Athens' : 665, 'Edinburgh' : 513, \n                        'Zurich' : 415, 'Oporto' : 240, 'Geneva' : 201, 'Krakow' : 769, \n                        'Oslo' : 681, 'Helsinki' : 643, 'Bratislava' : 426, \n                        'Luxembourg' : 119, 'Ljubljana' : 284}\n    df_output['CityPop'] = df_output['City'].apply(lambda x: СityPop_dict[x])\n\n    # Создаем признак - государство, и делаем из него Dummy-переменные\n    Country_dict = {'Paris' : 'FR', 'Stockholm' : 'SW', 'London' : 'UK', 'Berlin': 'D', 'Munich' : 'D', 'Oporto': 'PT',\n       'Milan' : 'IT', 'Bratislava' : 'SL', 'Vienna' : 'A', 'Rome' : 'IT', 'Barcelona' : 'S', 'Madrid' : 'S',\n       'Dublin' : 'IE', 'Brussels' : 'BE', 'Zurich' : 'CH', 'Warsaw' : 'PL', 'Budapest' : 'HU', 'Copenhagen': 'DK',\n       'Amsterdam' : 'NL', 'Lyon' : 'F', 'Hamburg' : 'D', 'Lisbon' : 'PT', 'Prague' : 'CR', 'Oslo' : 'NO',\n       'Helsinki': 'FI', 'Edinburgh': 'UK', 'Geneva' : 'CH' , 'Ljubljana' : 'SV', 'Athens' : 'GR',\n       'Luxembourg':  'LB', 'Krakow' : 'PL'}\n    df_output['Country'] = df_output['City'].apply(lambda x: Country_dict[x])\n    df_output = pd.concat([df_output, pd.get_dummies(df_output['Country'])], axis=1)\n    \n    # Отнормируем критерий Ranking по городам\n    meanRankCity = df_output.groupby(['City'])['Ranking'].mean()\n    countRestCity = df_output['City'].value_counts(ascending=False)\n    df_output['meanRankCity'] = df_output['City'].apply(lambda x: meanRankCity[x])\n    # Количество ресторанов в городе\n    df_output['countRestCity'] = df_output['City'].apply(lambda x: countRestCity[x])\n    # Нормализуем Ranking относительно количества ресторанов в городе\n    df_output['normRankRestCity'] = (df_output['Ranking'] - df_output['meanRankCity']) / df_output['countRestCity']\n    \n    # Признак - количество ресторанов в городе на тысячу жителей\n    df_output['RestCityDensity'] = df_output['countRestCity'] / df_output['CityPop']\n    #df_output['normCityPop'] =  df_output['CityPop'] / df_output['countRestCity']\n\n    \n    # Новый признак - количество отзывов на тысячу жителей\n    df_output['Number of Reviews'] = df_output['Number of Reviews'].fillna(0)\n    df_output['ReviewsPerPop'] = df_output['Number of Reviews'] / df_output['CityPop']\n    \n    # Новый признак - среднее количество отзывов в городе\n    ReviewsByCity = df_output.groupby(\"City\")[\"Number of Reviews\"].mean()\n    df_output[\"ReviewsByCity\"] = df_output[\"City\"].apply(\n        lambda x: ReviewsByCity[x])\n    \n    \n\n    # Убираем ненужные признаки\n    df_output = df_output.drop(['meanRankCity', 'CityPop'], axis = 1)\n\n    # Создаем список всех возможных кухонь.\n    dfc = (df_output['Cuisine Style'].explode()).reset_index()\n    dfcl = list(dfc['Cuisine Style'].unique())\n    \n    # Заменяем список кухонь для ресторана на Dummy-переменные со стилем кухни.\n    def find_item(cell):\n        if item in cell:\n            return 1\n        return 0\n    for item in dfcl:\n        df_output[item] = df_output['Cuisine Style'].apply(find_item)\n        \n    # Функция расчета количества кухонь в городе\n    def GetCuisinesByCity(data):\n        GroupCity = data.explode(\"Cuisine Style\").groupby(\"City\")[\n            \"Cuisine Style\"]\n        return GroupCity.aggregate(lambda s: len(set(s.values))).sort_values(ascending=False)\n    \n    # Новый признак - количество видов кухни в городе\n    CuisinesByCity = GetCuisinesByCity(df_output)\n    df_output[\"CuisinesByCity\"] = df_output[\"City\"].apply(lambda s: CuisinesByCity[s])\n    \n    \n    # Попробуем найти, предлагает ли ресторан кухню из N самых популярных\n    # Функция определяет, есть ли в ресторане кухня из списка популярных\n    def IsInPopular(cell):\n        for element in list(cell):\n            if element in MostPopCuis:\n                return 1\n        return 0\n    # Новый признак - предлагается кухня из N популярных\n    N = 5\n    MostPopCuis = list(dfc['Cuisine Style'].value_counts().keys()[0:N+1])\n    MostPopCuis.remove('Unknown')\n    df_output['MostPopCuis'] = df_output['Cuisine Style'].apply(IsInPopular)\n    \n    \n    # Попробуем найти, предлагает ли ресторан уникальную кухню (встречается меньше чем в N1 ресторанах) \n    # Функция определяет, есть ли в ресторане кухня из списка уникальных\n    def IsInUnique(cell):\n        for element in list(cell):\n            if element in UniqueList:\n                return 1\n        return 0\n    # Новый признак - предлагается кухня из уникальных\n    N1 = 100\n    CuisineList = dfc['Cuisine Style'].value_counts()\n    UniqueList = CuisineList.where(CuisineList <= N1 ).dropna().keys()\n\n    df_output['UniqueCuis'] = df_output['Cuisine Style'].apply(IsInUnique)\n\n    \n    # Очищаем строки с отзывами и превращаем отзывы в списки, пропущенные даты заменяем на 01/01/1900\n    df_output['Reviews'].fillna('[[], []]', inplace=True)\n    def RewFill(s):\n        s = s.replace(\"nan]\", \"'']\")\n        s = s.replace(\"[nan\", \"[''\")\n        if len(s) < 15:\n            return \"[['',''], ['01/01/1900','01/01/1900']]\"\n        return s\n    df_output['Reviews'] = df_output['Reviews'].apply(RewFill)\n    df_output['Reviews'] = df_output['Reviews'].apply(lambda s: eval(s))\n    \n    # Создаем новые признаки - даты первого и второго отзыва\n    df_output['Ans1'] = pd.to_datetime(df_output['Reviews'].apply(lambda x: x[1][0]))\n    df_output['Ans2'] = pd.to_datetime(df_output['Reviews'].apply(lambda x: x[1][1] if (len(x[1]) > 1) else '01/01/1900'))\n    \n    # Заменяем пропущенные даты (01/01/1900) на среднее значение даты среди всех отзывов, округленное до дней\n    M1min = df_output['Ans1'].min()\n    M1 = df_output[df_output['Ans1'] != M1min]['Ans1'].mean().round('D')\n    df_output['Ans1'] = df_output['Ans1'].apply(lambda x: x if x > M1min else M1)\n    M2min = df_output['Ans2'].min().round('D')\n    M2 = df_output[df_output['Ans2'] != M2min]['Ans2'].mean().round('D')\n    df_output['Ans2'] = df_output['Ans2'].apply(lambda x: x if x > M2min else M2)\n\n    # Заменяем дату на количество дней от даты отзыва до текущей даты\n    today = pd.to_datetime(\"today\").round('D')\n    df_output['Ans1'] = df_output['Ans1'].apply(lambda x: (today - x).days )\n    df_output['Ans2'] = df_output['Ans2'].apply(lambda x: (today - x).days )\n    \n    # Вводим новую переменную - количество дней между отзывами.\n    df_output['DAns'] = (df_output['Ans1'] - df_output['Ans2']).apply(abs)\n\n\n    \n    #df_output['Number of Reviews'] = df_output['Number of Reviews'].fillna(df_output['Number of Reviews'].mean())\n    #df_output['Number of Reviews'] = df_output['Number of Reviews'].round(-1)\n\n    #df_output = df_output.drop(['City', 'Cuisine Style', 'Price Range', 'Reviews', 'URL_TA', 'ID_TA'], axis = 1)\n\n    #df_output = df_output.drop(['Price_N' ], axis = 1)\n    \n    \n    \n    \n    \n    # ################### 5. Clean #################################################### \n    # убираем признаки которые еще не успели обработать, \n    # модель на признаках с dtypes \"object\" обучаться не будет, просто выберим их и удалим\n    object_columns = [s for s in df_output.columns if df_output[s].dtypes == 'object']\n    df_output.drop(object_columns, axis = 1, inplace=True)\n    \n    return df_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">По хорошему, можно было бы перевести эту большую функцию в класс и разбить на подфункции (согласно ООП). "},{"metadata":{},"cell_type":"markdown","source":"#### Запускаем и проверяем что получилось"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc = preproc_data(data)\ndf_preproc.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # наш таргет\nX = train_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \nЭто поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model \nСам ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nЕсли все устраевает - готовим Submission на кагл"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What's next?\nИли что делать, чтоб улучшить результат:\n* Обработать оставшиеся признаки в понятный для машины формат\n* Посмотреть, что еще можно извлечь из признаков\n* Сгенерировать новые признаки\n* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n* Подобрать состав признаков\n\nВ общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}